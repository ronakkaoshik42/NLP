{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-2-17110068-RonakKaoshik.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Glr1JHMgvolG",
        "colab_type": "text"
      },
      "source": [
        "# **Classical Approach**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27f23d2JE6z8",
        "colab_type": "code",
        "outputId": "d442122f-1af6-4fd9-edc2-8c5db325b6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "cd sample_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'sample_data'\n",
            "/content/sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzBF4feqYZ_1",
        "colab_type": "code",
        "outputId": "b41cd6c2-3343-476f-9415-52476a7a2dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "pip install nltk==3.4"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.4 in /usr/local/lib/python3.6/dist-packages (3.4)\n",
            "Requirement already satisfied: singledispatch in /usr/local/lib/python3.6/dist-packages (from nltk==3.4) (3.4.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLwywGXrwDDj",
        "colab_type": "text"
      },
      "source": [
        "**Data pre-processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4egkMrJvUr4",
        "colab_type": "code",
        "outputId": "724f0112-25ba-4559-9afa-64b213e9c2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import io\n",
        "import os\n",
        "import requests\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "url = \"https://raw.githubusercontent.com/ronakkaoshik42/NLP/master/jane.txt\"\n",
        "text = requests.get(url).content.decode('utf8')\n",
        "with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
        "        fout.write(text)\n",
        "print(len(text))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "4452799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiLjCykSwIJB",
        "colab_type": "text"
      },
      "source": [
        "**Procuring meaningful and complete text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5QbddvVya-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "s=re.sub('Chapter [0-9]+','',text)\n",
        "s=re.sub('Esq.','Esq',text)\n",
        "s=s.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJRT94o9wWJl",
        "colab_type": "text"
      },
      "source": [
        "**Generating sentence tokens, followed by word tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIabkGnb3ddB",
        "colab_type": "code",
        "outputId": "a1890e73-24ce-43c3-ef21-7ef594f7ce85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.util import pad_sequence\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "sent_tokenize_list = sent_tokenize(s)\n",
        "for i in range(len(sent_tokenize_list)):\n",
        "  #sent_tokenize_list[i]=re.sub(\"[^\\w\\s]\",\"\",sent_tokenize_list[i])\n",
        "  sent_tokenize_list[i]=re.sub(\"\\\\r\\\\n\",\" \",sent_tokenize_list[i])\n",
        "g=[]\n",
        "for i in range(len(sent_tokenize_list)):\n",
        "  g.append(word_tokenize(sent_tokenize_list[i]))\n",
        "tokens=0\n",
        "for i in range(len(g)):\n",
        "  tokens=tokens+len(g[i])\n",
        "print('Total number of tokens in given dataset-->',tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "924146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLlrPIBewquz",
        "colab_type": "text"
      },
      "source": [
        "**Splitting dataset for test and train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9jBSlB14Q_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "x=np.array(g)\n",
        "x_train, x_test= train_test_split(g,test_size=0.2,random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNQwgXleEmbG",
        "colab_type": "text"
      },
      "source": [
        "**Compute MLE for unigram, bigram, trigrams, and quadgrams. How many n-grams are\n",
        "possible and how many actually exist? Use the training corpus and NLTK library.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOT62C278nue",
        "colab_type": "code",
        "outputId": "466146f3-8a61-4e31-92d9-0355242a57b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from collections import Counter\n",
        "l=[]\n",
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "for i in range(len(x_train)):\n",
        "  l.append(list(pad_both_ends(x_train[i], n=2)))#padding the sentences for start end marking\n",
        "#detokenizing from sentence to words\n",
        "flist=[]\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(l[i])):\n",
        "    flist.append(l[i][j])\n",
        "#For unigrams\n",
        "unigrams=Counter(flist)\n",
        "c=0\n",
        "for i in unigrams.keys():\n",
        "    c=c+unigrams[i]\n",
        "print('Unigrams Count, i.e, Vocabulary count -->',len(unigrams))\n",
        "V=len(unigrams)\n",
        "print('Theoretical Unigram count-->',V)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unigrams Count, i.e, Vocabulary count --> 15581\n",
            "Theoretical Unigram count--> 15581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg2n-g7etVVQ",
        "colab_type": "code",
        "outputId": "c59ab33b-f76d-4da0-fcd5-bd02db67f75a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#MLE for unigrams\n",
        "unigrams_mle={} #Dictionary for storing MLE values of unigrams\n",
        "for i in unigrams.keys():\n",
        "    unigrams_mle[i]=unigrams[i]/tokens\n",
        "print(\"The MLE for the word 'own' is-->\",unigrams_mle[\"own\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The MLE for the word 'own' is--> 0.0012790186831950795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL6QWYBJ8xC5",
        "colab_type": "text"
      },
      "source": [
        "Sentence generator for unigram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Si7c5vZ46yzc",
        "colab_type": "code",
        "outputId": "91aee268-9df9-46af-d13f-a4d01b4ae157",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.lm import MLE\n",
        "n = 1\n",
        "x_train1, padded_sents1 = padded_everygram_pipeline(n, g)\n",
        "model1 = MLE(n)\n",
        "len(model1.vocab)\n",
        "model1.fit(x_train1, padded_sents1)\n",
        "print(model1.vocab)\n",
        "len(model1.vocab)\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "def Generator(model, num_words, random_seed=42): # here num_words is the max word count\n",
        "    x = []\n",
        "    for i in model.generate(num_words, random_seed=random_seed):\n",
        "        if i == '<s>':\n",
        "            continue\n",
        "        if i == '</s>':\n",
        "            break\n",
        "        x.append(i)\n",
        "    return detokenize(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 16941 items>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s4oIcaA8c0_",
        "colab_type": "code",
        "outputId": "e0d08977-601b-4b98-941f-f888dc4d12bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "generate_sent(model1, 3, random_seed=7)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'curricle curricle curricle'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO6WPcVM85Pc",
        "colab_type": "text"
      },
      "source": [
        "As can be seen above, the generated sentence has repetitive terms. So unigram not much useful here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT_ZDDDP4grC",
        "colab_type": "code",
        "outputId": "e9a45694-1fb6-4f42-a0f3-098f76d17bb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#For bigrams\n",
        "bigrams=Counter() #Stores the bigram counts for all the possible combinations from given dataset\n",
        "for i in range(len(flist)-1):\n",
        "    b=(flist[i],flist[i+1])\n",
        "    bigrams[b]=bigrams.get(b,0)+1\n",
        "print('Bigrams Count-->',len(bigrams))\n",
        "print('Theoretical Bigram count-->',V**2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bigrams Count--> 176793\n",
            "Theoretical Bigram count--> 242767561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOxYLJFPObm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLE for bigrams\n",
        "bigrams_mle={}\n",
        "for (i,j) in bigrams.keys():\n",
        "    bigrams_mle[(i,j)]=bigrams[(i,j)]/unigrams[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNi3YEIc9H84",
        "colab_type": "code",
        "outputId": "1459dcee-177f-4240-fba1-f4842bbe74a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "n = 2\n",
        "x_train2, padded_sents2 = padded_everygram_pipeline(n, g)\n",
        "model2 = MLE(n)\n",
        "len(model2.vocab)\n",
        "model2.fit(x_train2, padded_sents2)\n",
        "print(model2.vocab)\n",
        "len(model2.vocab)\n",
        "Generator(model2, 12, random_seed=23)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 16943 items>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'very well take up with us to-day on to think what will'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aso0m2lc9ggb",
        "colab_type": "text"
      },
      "source": [
        "As can be seen above, the sentence generated using bigram models is much more better than previous one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQHdO_0ZSC7j",
        "colab_type": "code",
        "outputId": "6cedbff0-73a8-4fcc-d4e8-0bb63f10bd13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#For trigrams\n",
        "trigrams=Counter()\n",
        "for i in range(len(flist)-2):\n",
        "    b=(flist[i],flist[i+1],flist[i+2])\n",
        "    trigrams[b]=trigrams.get(b,0)+1\n",
        "print('Trigrams Count-->',len(trigrams))\n",
        "print('Theoretical Trigram count-->',V**3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigrams Count--> 451217\n",
            "Theoretical Trigram count--> 3782561367941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I2jb9xk47-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLE for trigrams\n",
        "trigrams_mle={}\n",
        "for (i,j,k) in trigrams.keys():\n",
        "    trigrams_mle[(i,j,k)]=trigrams[(i,j,k)]/bigrams[(i,j)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXRpRRrS9xHF",
        "colab_type": "code",
        "outputId": "802a929e-a517-4592-bf67-c44be86f4a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "n = 3\n",
        "x_train3, padded_sents3 = padded_everygram_pipeline(n, g)\n",
        "model3 = MLE(n)\n",
        "len(model3.vocab)\n",
        "model3.fit(x_train3, padded_sents3)\n",
        "print(model3.vocab)\n",
        "len(model3.vocab)\n",
        "Generator(model3, 12, random_seed=45)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 16943 items>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a gentleman, and happiness.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6boG4a89SvdM",
        "colab_type": "code",
        "outputId": "18901ce2-2dea-43b1-fd7f-5bf9fab3b3e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#For quadgrams\n",
        "quadgrams=Counter()\n",
        "for i in range(len(flist)-3):\n",
        "    b=(flist[i],flist[i+1],flist[i+2],flist[i+3])\n",
        "    quadgrams[b]=quadgrams.get(b,0)+1\n",
        "print('Quadgrams Count-->',len(quadgrams))\n",
        "print('Theoretical Quadgram count-->',V**3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quadgrams Count--> 631197\n",
            "Theoretical Quadgram count--> 3782561367941\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFbs1BiZS4PE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#MLE for quadgrams\n",
        "quadgrams_mle={}\n",
        "for (i,j,k,l) in quadgrams.keys():\n",
        "    quadgrams_mle[(i,j,k,l)]=quadgrams[(i,j,k,l)]/trigrams[(i,j,k)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGkjO7MC_VJ4",
        "colab_type": "code",
        "outputId": "f5ac6102-6d65-48eb-a878-13a6739c772b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "n = 4\n",
        "x_train4, padded_sents4 = padded_everygram_pipeline(n,g)\n",
        "model4 = MLE(n)\n",
        "len(model4.vocab)\n",
        "model4.fit(x_train4, padded_sents4)\n",
        "print(model4.vocab)\n",
        "len(model4.vocab)\n",
        "Generator(model4, 12, random_seed=37)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 16943 items>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mrs. norris had not the smallest objection to his marrying their daughter'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjv-OWCgAlPr",
        "colab_type": "text"
      },
      "source": [
        "The perplexity on the test dataset will turn out to be infinity without smoothing. Hence applying add 1 and good turing smoothing to bigram model and calculating the respective perplexity as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYCs2Y6foYBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigrams_mle1={}\n",
        "bigrams1={} #effective count after smoothing\n",
        "for (i,j) in bigrams.keys():\n",
        "    bigrams_mle1[(i,j)]=(bigrams[(i,j)]+1)/(unigrams[i]+V) #Add-1\n",
        "    bigrams1[(i,j)]=unigrams[i]*bigrams_mle1[(i,j)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQdLIqKTqHNY",
        "colab_type": "code",
        "outputId": "8803b422-9d4e-45c9-d740-4dcbdc5fde51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "test_words=x_test\n",
        "def perplexityadd1(test_words,big_mle1):\n",
        "    p=1\n",
        "    for i in range(len(test_words)-2):\n",
        "        prob=1/(unigrams[i]+V)\n",
        "        p*=(1/prob_w)**(1/len(test_words))\n",
        "    return p\n",
        "print(\"The perplexity for add 1 smoothing is-->\",perplexityadd1(test_words,big_mle1))\n",
        "def perplexityturing(test_words,d,bigrams):\n",
        "    p=1\n",
        "    for i in range(len(test_words)-2):\n",
        "        try:\n",
        "            b=(test_words[i],test_words[i+1])\n",
        "            c=bigrams[b]\n",
        "            if(c>d):\n",
        "                cstar=c-d\n",
        "            else:\n",
        "                cstar=d-c\n",
        "            prob_w=cstar/924146 #It is the total tokens\n",
        "        p*=(1/prob_w)**(1/len(test_words))\n",
        "    return p\n",
        "print(\"The perplexity for good turing smoothing is -->\",perplexityturing(test_words,0.780,bigrams))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The perplexity for add 1 smoothing is--> 155.38144359579456\n",
            "The perplexity for good turing smoothing is --> 92.05312200539007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMScBWblrfq-",
        "colab_type": "text"
      },
      "source": [
        "# Neural **Approac**h"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt6hVg71qMkr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be7e5b24-c2c7-42ef-9d01-6974856d3f12"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import RNN\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ywfWbmoro8K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "characters = sorted(list(set(text)))\n",
        "\n",
        "n_to_char = {n:char for n, char in enumerate(characters)}\n",
        "char_to_n = {char:n for n, char in enumerate(characters)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LY6cNhPrzE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3daa7172-d8b2-48fc-a44c-a4fabbf65988"
      },
      "source": [
        "vocab_size = len(characters)\n",
        "print('Number of unique characters: ', vocab_size)\n",
        "print(characters)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique characters:  85\n",
            "['\\n', '\\r', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubOGUBOhr1dN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = []   # extracted sequences\n",
        "Y = []   # the target - the follow up character\n",
        "length = len(text)\n",
        "seq_length = 100   #number of characters to consider before predicting the following character"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOp127RTr3FM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bbca773-5ac6-456b-e09c-806979394d70"
      },
      "source": [
        "for i in range(0, length - seq_length, 1):\n",
        "    sequence = text[i:i + seq_length]\n",
        "    label = text[i + seq_length]\n",
        "    X.append([char_to_n[char] for char in sequence])\n",
        "    Y.append(char_to_n[label])\n",
        "    \n",
        "print('Number of extracted sequences:', len(X))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of extracted sequences: 4452699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abd8EIVhr5d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_modified = np.reshape(X, (len(X), seq_length, 1))\n",
        "X_modified = X_modified / float(len(characters))\n",
        "Y_modified = np_utils.to_categorical(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOA_SCNVr9HA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1149a845-ef5d-47a9-9509-4b7fe0bb9e96"
      },
      "source": [
        "X_modified.shape, Y_modified.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4452699, 100, 1), (4452699, 85))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scKDiRT0r-iZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2aacb38-05ca-444d-a741-c6da9613c3e8"
      },
      "source": [
        "print(\"X[0].shape = {}, Y[0].shape = {}\".format(X_modified[0].shape, Y_modified[0].shape))\n",
        "print(\"X[0]: \", X_modified[0])\n",
        "print(\"Y[0]: \", Y_modified[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X[0].shape = (100, 1), Y[0].shape = (85,)\n",
            "X[0]:  [[0.37647059]\n",
            " [0.77647059]\n",
            " [0.69411765]\n",
            " [0.87058824]\n",
            " [0.91764706]\n",
            " [0.74117647]\n",
            " [0.89411765]\n",
            " [0.02352941]\n",
            " [0.2       ]\n",
            " [0.01176471]\n",
            " [0.        ]\n",
            " [0.01176471]\n",
            " [0.        ]\n",
            " [0.01176471]\n",
            " [0.        ]\n",
            " [0.56470588]\n",
            " [0.78823529]\n",
            " [0.89411765]\n",
            " [0.02352941]\n",
            " [0.61176471]\n",
            " [0.69411765]\n",
            " [0.82352941]\n",
            " [0.91764706]\n",
            " [0.74117647]\n",
            " [0.89411765]\n",
            " [0.02352941]\n",
            " [0.4       ]\n",
            " [0.82352941]\n",
            " [0.82352941]\n",
            " [0.78823529]\n",
            " [0.85882353]\n",
            " [0.91764706]\n",
            " [0.14117647]\n",
            " [0.02352941]\n",
            " [0.85882353]\n",
            " [0.75294118]\n",
            " [0.02352941]\n",
            " [0.47058824]\n",
            " [0.74117647]\n",
            " [0.82352941]\n",
            " [0.82352941]\n",
            " [0.97647059]\n",
            " [0.84705882]\n",
            " [0.71764706]\n",
            " [0.77647059]\n",
            " [0.02352941]\n",
            " [0.43529412]\n",
            " [0.69411765]\n",
            " [0.82352941]\n",
            " [0.82352941]\n",
            " [0.14117647]\n",
            " [0.02352941]\n",
            " [0.78823529]\n",
            " [0.84705882]\n",
            " [0.02352941]\n",
            " [0.56470588]\n",
            " [0.85882353]\n",
            " [0.83529412]\n",
            " [0.74117647]\n",
            " [0.89411765]\n",
            " [0.90588235]\n",
            " [0.74117647]\n",
            " [0.91764706]\n",
            " [0.90588235]\n",
            " [0.77647059]\n",
            " [0.78823529]\n",
            " [0.89411765]\n",
            " [0.74117647]\n",
            " [0.14117647]\n",
            " [0.02352941]\n",
            " [0.95294118]\n",
            " [0.69411765]\n",
            " [0.90588235]\n",
            " [0.02352941]\n",
            " [0.69411765]\n",
            " [0.02352941]\n",
            " [0.83529412]\n",
            " [0.69411765]\n",
            " [0.84705882]\n",
            " [0.02352941]\n",
            " [0.95294118]\n",
            " [0.77647059]\n",
            " [0.85882353]\n",
            " [0.14117647]\n",
            " [0.01176471]\n",
            " [0.        ]\n",
            " [0.75294118]\n",
            " [0.85882353]\n",
            " [0.89411765]\n",
            " [0.02352941]\n",
            " [0.77647059]\n",
            " [0.78823529]\n",
            " [0.90588235]\n",
            " [0.02352941]\n",
            " [0.85882353]\n",
            " [0.95294118]\n",
            " [0.84705882]\n",
            " [0.02352941]\n",
            " [0.69411765]\n",
            " [0.83529412]]\n",
            "Y[0]:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ1Q9WoasAOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(400, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(400))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(Y_modified.shape[1], activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyUW-TCrKtA3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d25739bd-c34e-45ce-ffa7-0692ca8f6d87"
      },
      "source": [
        "ls"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "baseline-improvement-06-0.9927.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGw1wZ5FsP4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the network weights\n",
        "filename = \"baseline-improvement-06-0.9927.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upx0OA2OM893",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5bad327-6804-439a-932e-2470a46f0193"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld3xu5jUsSCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filepath=\"model_weights/baseline-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqg8JkL2siki",
        "colab_type": "code",
        "outputId": "c58d263c-2274-455f-cac0-722ea98f63cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "model.fit(X_modified, Y_modified, epochs=2, batch_size=128, callbacks = callbacks_list)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "286904/286904 [==============================] - 949s 3ms/step - loss: 1.2354\n",
            "\n",
            "Epoch 00001: loss improved from inf to 1.23540, saving model to model_weights/baseline-improvement-01-1.2354.hdf5\n",
            "Epoch 2/2\n",
            "286904/286904 [==============================] - 949s 3ms/step - loss: 1.1685\n",
            "\n",
            "Epoch 00002: loss improved from 1.06508 to 0.99274, saving model to model_weights/baseline-improvement-02-0.9927.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHq6aFCXsnJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start = np.random.randint(0, len(X)-1) # or generate random start\n",
        "\n",
        "string_mapped = list(X[start])\n",
        "\n",
        "full_string = [n_to_char[value] for value in string_mapped]\n",
        "\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join(full_string), \"\\\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz3f1K0iP4J2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(100):\n",
        "    x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
        "    x = x / float(len(characters))\n",
        "\n",
        "    pred_index = np.argmax(model.predict(x, verbose=0))\n",
        "    seq = [n_to_char[value] for value in string_mapped]\n",
        "    full_string.append(n_to_char[pred_index])\n",
        "    \n",
        "    string_mapped.append(pred_index)  # add the predicted character to the end\n",
        "    string_mapped = string_mapped[1:len(string_mapped)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_6x22NOP8k5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "txt=\"\"\n",
        "for char in full_string:\n",
        "    txt = txt+char"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc6_OEOlP-8l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3ea79ac9-8b27-4dd9-ad84-ad2cc7e7996a"
      },
      "source": [
        "print(start)\n",
        "print(txt)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "elizabeth did not quite equal her father in personal contentment. always to be presented with the date had had a disappointment. he had not been known to them as a boy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG2WiALLRp8d",
        "colab_type": "text"
      },
      "source": [
        "# **Comparison**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxeT7jOsRy73",
        "colab_type": "text"
      },
      "source": [
        "As is observed above, the text generated by this LSTM based architecture is much better grammatically when compared to the sentences generated by the n-gram models. However, the quadgram is quite close. (Will compare the perplexity scores and upload in a later version of the assignment, post deadline)"
      ]
    }
  ]
}